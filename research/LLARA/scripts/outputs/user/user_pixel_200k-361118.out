[2024-11-10 22:28:12,660] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /home/jingyuah/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2024-11-10 22:28:12,851] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-10 22:28:12,870] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-10 22:28:12,881] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
Warning: The default cache directory for DeepSpeed Triton autotune, /home/jingyuah/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /home/jingyuah/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /home/jingyuah/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-11-10 22:28:14,556] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-10 22:28:14,557] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-10 22:28:14,557] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-10 22:28:14,558] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-10 22:28:14,628] [INFO] [comm.py:637:init_distributed] cdb=None
both task... 
Loading from pretrained model_name_or_path: /data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T
both task... 
Loading from pretrained model_name_or_path: /data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T
both task... 
Loading from pretrained model_name_or_path: /data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T
both task... 
Loading from pretrained model_name_or_path: /data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T
LlamaTokenizer(name_or_path='/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
LlamaTokenizer(name_or_path='/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
LlamaTokenizer(name_or_path='/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
LlamaTokenizer(name_or_path='/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
LlamaTokenizer(name_or_path='/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>', 'additional_special_tokens': ['<s1>', '<s2>', '<s3>', '<s4>', '<s5>', '<s6>', '<s7>', '<s8>', '<s9>', '<s10>', '<s11>', '<s12>', '<s13>', '<s14>', '<s15>', '<s16>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32000: AddedToken("<s1>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32001: AddedToken("<s2>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32002: AddedToken("<s3>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32003: AddedToken("<s4>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32004: AddedToken("<s5>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32005: AddedToken("<s6>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32006: AddedToken("<s7>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32007: AddedToken("<s8>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32008: AddedToken("<s9>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32009: AddedToken("<s10>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32010: AddedToken("<s11>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32011: AddedToken("<s12>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32012: AddedToken("<s13>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32013: AddedToken("<s14>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32014: AddedToken("<s15>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32015: AddedToken("<s16>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
suffix:  ['", compress the above item sequence into embeddings: <unk>', '", predict the next item embedding: <unk>']
suffix:  ['", compress the above item sequence into embeddings: <unk>', '", predict the next item embedding: <unk>']
LlamaTokenizer(name_or_path='/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>', 'additional_special_tokens': ['<s1>', '<s2>', '<s3>', '<s4>', '<s5>', '<s6>', '<s7>', '<s8>', '<s9>', '<s10>', '<s11>', '<s12>', '<s13>', '<s14>', '<s15>', '<s16>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32000: AddedToken("<s1>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32001: AddedToken("<s2>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32002: AddedToken("<s3>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32003: AddedToken("<s4>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32004: AddedToken("<s5>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32005: AddedToken("<s6>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32006: AddedToken("<s7>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32007: AddedToken("<s8>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32008: AddedToken("<s9>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32009: AddedToken("<s10>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32010: AddedToken("<s11>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32011: AddedToken("<s12>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32012: AddedToken("<s13>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32013: AddedToken("<s14>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32014: AddedToken("<s15>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32015: AddedToken("<s16>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
suffix:  ['", compress the above item sequence into embeddings: <unk>', '", predict the next item embedding: <unk>']
suffix:  ['", compress the above item sequence into embeddings: <unk>', '", predict the next item embedding: <unk>']
LlamaTokenizer(name_or_path='/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>', 'additional_special_tokens': ['<s1>', '<s2>', '<s3>', '<s4>', '<s5>', '<s6>', '<s7>', '<s8>', '<s9>', '<s10>', '<s11>', '<s12>', '<s13>', '<s14>', '<s15>', '<s16>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32000: AddedToken("<s1>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32001: AddedToken("<s2>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32002: AddedToken("<s3>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32003: AddedToken("<s4>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32004: AddedToken("<s5>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32005: AddedToken("<s6>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32006: AddedToken("<s7>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32007: AddedToken("<s8>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32008: AddedToken("<s9>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32009: AddedToken("<s10>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32010: AddedToken("<s11>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32011: AddedToken("<s12>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32012: AddedToken("<s13>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32013: AddedToken("<s14>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32014: AddedToken("<s15>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32015: AddedToken("<s16>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
suffix:  ['", compress the above item sequence into embeddings: <unk>', '", predict the next item embedding: <unk>']
suffix:  ['", compress the above item sequence into embeddings: <unk>', '", predict the next item embedding: <unk>']
LlamaTokenizer(name_or_path='/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>', 'additional_special_tokens': ['<s1>', '<s2>', '<s3>', '<s4>', '<s5>', '<s6>', '<s7>', '<s8>', '<s9>', '<s10>', '<s11>', '<s12>', '<s13>', '<s14>', '<s15>', '<s16>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32000: AddedToken("<s1>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32001: AddedToken("<s2>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32002: AddedToken("<s3>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32003: AddedToken("<s4>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32004: AddedToken("<s5>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32005: AddedToken("<s6>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32006: AddedToken("<s7>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32007: AddedToken("<s8>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32008: AddedToken("<s9>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32009: AddedToken("<s10>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32010: AddedToken("<s11>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32011: AddedToken("<s12>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32012: AddedToken("<s13>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32013: AddedToken("<s14>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32014: AddedToken("<s15>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	32015: AddedToken("<s16>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
suffix:  ['", compress the above item sequence into embeddings: <unk>', '", predict the next item embedding: <unk>']
suffix:  ['", compress the above item sequence into embeddings: <unk>', '", predict the next item embedding: <unk>']
{'loss': 3.9935, 'grad_norm': 1.4179654121398926, 'learning_rate': 5.357103623792504e-06, 'epoch': 0.02}
{'loss': 3.5855, 'grad_norm': 1.4760429859161377, 'learning_rate': 6.306295708718102e-06, 'epoch': 0.03}
{'loss': 3.4222, 'grad_norm': 1.628466248512268, 'learning_rate': 6.861537484380907e-06, 'epoch': 0.05}
{'loss': 3.2785, 'grad_norm': 1.7455008029937744, 'learning_rate': 7.255487793643697e-06, 'epoch': 0.07}
{'loss': 3.1518, 'grad_norm': 2.1994433403015137, 'learning_rate': 7.5610593932259595e-06, 'epoch': 0.08}
{'loss': 3.0059, 'grad_norm': 3.161536931991577, 'learning_rate': 7.810729569306503e-06, 'epoch': 0.1}
{'loss': 2.8706, 'grad_norm': 4.296815872192383, 'learning_rate': 8.021822695386498e-06, 'epoch': 0.12}
{'loss': 2.755, 'grad_norm': 3.7897274494171143, 'learning_rate': 8.204679878569296e-06, 'epoch': 0.13}
{'loss': 2.6202, 'grad_norm': 5.506801605224609, 'learning_rate': 8.36597134496931e-06, 'epoch': 0.15}
{'loss': 2.5061, 'grad_norm': 4.844742774963379, 'learning_rate': 8.510251478151555e-06, 'epoch': 0.17}
{'loss': 2.4108, 'grad_norm': 5.8577165603637695, 'learning_rate': 8.640768734544373e-06, 'epoch': 0.19}
{'loss': 2.2905, 'grad_norm': 9.648904800415039, 'learning_rate': 8.7599216542321e-06, 'epoch': 0.2}
{'loss': 2.1982, 'grad_norm': 4.902595520019531, 'learning_rate': 8.869531714996335e-06, 'epoch': 0.22}
{'loss': 2.1158, 'grad_norm': 4.9186859130859375, 'learning_rate': 8.971014780312095e-06, 'epoch': 0.24}
{'loss': 2.0251, 'grad_norm': 5.682520389556885, 'learning_rate': 9.065493253814362e-06, 'epoch': 0.25}
{'loss': 1.941, 'grad_norm': 5.322051525115967, 'learning_rate': 9.153871963494891e-06, 'epoch': 0.27}
{'loss': 1.8571, 'grad_norm': 7.334187030792236, 'learning_rate': 9.236891000134819e-06, 'epoch': 0.29}
{'loss': 1.8117, 'grad_norm': 6.116108417510986, 'learning_rate': 9.315163429894905e-06, 'epoch': 0.3}
{'loss': 1.7904, 'grad_norm': 6.976386070251465, 'learning_rate': 9.389202796890828e-06, 'epoch': 0.32}
{'loss': 1.7198, 'grad_norm': 7.073964595794678, 'learning_rate': 9.459443563077151e-06, 'epoch': 0.34}
{'eval_loss': 1.6765167713165283, 'eval_runtime': 176.7916, 'eval_samples_per_second': 56.564, 'eval_steps_per_second': 0.888, 'epoch': 0.34}
{'loss': 1.6547, 'grad_norm': 6.684567451477051, 'learning_rate': 9.5262565559749e-06, 'epoch': 0.35}
{'loss': 1.6187, 'grad_norm': 5.286545276641846, 'learning_rate': 9.589960819469969e-06, 'epoch': 0.37}
{'loss': 1.5739, 'grad_norm': 5.343843460083008, 'learning_rate': 9.650832828152373e-06, 'epoch': 0.39}
{'loss': 1.5209, 'grad_norm': 6.529822826385498, 'learning_rate': 9.709113739157698e-06, 'epoch': 0.4}
{'loss': 1.4921, 'grad_norm': 5.110128879547119, 'learning_rate': 9.765015162659413e-06, 'epoch': 0.42}
{'loss': 1.4529, 'grad_norm': 5.474804878234863, 'learning_rate': 9.818723799921933e-06, 'epoch': 0.44}
{'loss': 1.4362, 'grad_norm': 5.424417972564697, 'learning_rate': 9.87040520555771e-06, 'epoch': 0.45}
{'loss': 1.404, 'grad_norm': 7.413949966430664, 'learning_rate': 9.920206865237691e-06, 'epoch': 0.47}
{'loss': 1.3814, 'grad_norm': 6.789379596710205, 'learning_rate': 9.96826073308657e-06, 'epoch': 0.49}
{'loss': 1.3561, 'grad_norm': 5.410920143127441, 'learning_rate': 9.988769092542678e-06, 'epoch': 0.51}
{'loss': 1.321, 'grad_norm': 6.261661529541016, 'learning_rate': 9.951332734351604e-06, 'epoch': 0.52}
{'loss': 1.3065, 'grad_norm': 5.6055989265441895, 'learning_rate': 9.913896376160528e-06, 'epoch': 0.54}
{'loss': 1.2835, 'grad_norm': 5.550662994384766, 'learning_rate': 9.876460017969453e-06, 'epoch': 0.56}
{'loss': 1.2675, 'grad_norm': 4.763644695281982, 'learning_rate': 9.839023659778377e-06, 'epoch': 0.57}
{'loss': 1.252, 'grad_norm': 5.0842790603637695, 'learning_rate': 9.801587301587301e-06, 'epoch': 0.59}
{'loss': 1.2295, 'grad_norm': 4.880959987640381, 'learning_rate': 9.764150943396227e-06, 'epoch': 0.61}
{'loss': 1.2143, 'grad_norm': 4.708102226257324, 'learning_rate': 9.726714585205152e-06, 'epoch': 0.62}
{'loss': 1.2004, 'grad_norm': 4.621988296508789, 'learning_rate': 9.689278227014076e-06, 'epoch': 0.64}
{'loss': 1.1996, 'grad_norm': 5.240365982055664, 'learning_rate': 9.651841868823002e-06, 'epoch': 0.66}
{'loss': 1.1814, 'grad_norm': 4.724034786224365, 'learning_rate': 9.614405510631926e-06, 'epoch': 0.67}
{'eval_loss': 1.1789569854736328, 'eval_runtime': 175.5526, 'eval_samples_per_second': 56.963, 'eval_steps_per_second': 0.894, 'epoch': 0.67}
{'loss': 1.1701, 'grad_norm': 4.734228134155273, 'learning_rate': 9.576969152440852e-06, 'epoch': 0.69}
{'loss': 1.1623, 'grad_norm': 5.019850730895996, 'learning_rate': 9.539532794249776e-06, 'epoch': 0.71}
{'loss': 1.158, 'grad_norm': 4.328199863433838, 'learning_rate': 9.502096436058701e-06, 'epoch': 0.72}
{'loss': 1.1457, 'grad_norm': 4.447011947631836, 'learning_rate': 9.464660077867625e-06, 'epoch': 0.74}
{'loss': 1.126, 'grad_norm': 4.504446506500244, 'learning_rate': 9.427223719676551e-06, 'epoch': 0.76}
{'loss': 1.1298, 'grad_norm': 4.421235084533691, 'learning_rate': 9.389787361485475e-06, 'epoch': 0.77}
{'loss': 1.1179, 'grad_norm': 4.182431221008301, 'learning_rate': 9.3523510032944e-06, 'epoch': 0.79}
{'loss': 1.1094, 'grad_norm': 4.391610622406006, 'learning_rate': 9.314914645103326e-06, 'epoch': 0.81}
{'loss': 1.1111, 'grad_norm': 4.4109296798706055, 'learning_rate': 9.27747828691225e-06, 'epoch': 0.83}
{'loss': 1.0932, 'grad_norm': 3.493828296661377, 'learning_rate': 9.240041928721176e-06, 'epoch': 0.84}
{'loss': 1.0895, 'grad_norm': 3.942678689956665, 'learning_rate': 9.2026055705301e-06, 'epoch': 0.86}
{'loss': 1.0772, 'grad_norm': 3.8267245292663574, 'learning_rate': 9.165169212339024e-06, 'epoch': 0.88}
{'loss': 1.0734, 'grad_norm': 4.990884304046631, 'learning_rate': 9.12773285414795e-06, 'epoch': 0.89}
{'loss': 1.069, 'grad_norm': 3.4667067527770996, 'learning_rate': 9.090296495956873e-06, 'epoch': 0.91}
{'loss': 1.0651, 'grad_norm': 3.617867946624756, 'learning_rate': 9.052860137765799e-06, 'epoch': 0.93}
{'loss': 1.057, 'grad_norm': 3.943204641342163, 'learning_rate': 9.015423779574723e-06, 'epoch': 0.94}
{'loss': 1.0582, 'grad_norm': 3.9446494579315186, 'learning_rate': 8.977987421383649e-06, 'epoch': 0.96}
{'loss': 1.0475, 'grad_norm': 4.072676658630371, 'learning_rate': 8.940551063192573e-06, 'epoch': 0.98}
{'loss': 1.0474, 'grad_norm': 3.706430673599243, 'learning_rate': 8.903114705001498e-06, 'epoch': 0.99}
{'loss': 1.0244, 'grad_norm': 3.5613391399383545, 'learning_rate': 8.865678346810424e-06, 'epoch': 1.01}
{'eval_loss': 1.0468709468841553, 'eval_runtime': 180.1665, 'eval_samples_per_second': 55.504, 'eval_steps_per_second': 0.871, 'epoch': 1.01}
{'loss': 1.0165, 'grad_norm': 3.499094009399414, 'learning_rate': 8.828241988619348e-06, 'epoch': 1.03}
{'loss': 1.0091, 'grad_norm': 3.2990076541900635, 'learning_rate': 8.790805630428273e-06, 'epoch': 1.04}
{'loss': 1.0105, 'grad_norm': 3.7914137840270996, 'learning_rate': 8.753369272237197e-06, 'epoch': 1.06}
{'loss': 1.0153, 'grad_norm': 2.903475761413574, 'learning_rate': 8.715932914046123e-06, 'epoch': 1.08}
{'loss': 1.0071, 'grad_norm': 3.330232620239258, 'learning_rate': 8.678496555855047e-06, 'epoch': 1.1}
{'loss': 1.0115, 'grad_norm': 3.228243350982666, 'learning_rate': 8.641060197663973e-06, 'epoch': 1.11}
{'loss': 1.0035, 'grad_norm': 2.9785542488098145, 'learning_rate': 8.603623839472897e-06, 'epoch': 1.13}
{'loss': 1.0023, 'grad_norm': 3.2483599185943604, 'learning_rate': 8.56618748128182e-06, 'epoch': 1.15}
{'loss': 0.995, 'grad_norm': 2.6849160194396973, 'learning_rate': 8.528751123090746e-06, 'epoch': 1.16}
{'loss': 0.9961, 'grad_norm': 3.968953847885132, 'learning_rate': 8.49131476489967e-06, 'epoch': 1.18}
{'loss': 0.9965, 'grad_norm': 3.1572678089141846, 'learning_rate': 8.453878406708596e-06, 'epoch': 1.2}
{'loss': 0.9986, 'grad_norm': 3.0721302032470703, 'learning_rate': 8.416442048517522e-06, 'epoch': 1.21}
{'loss': 0.9939, 'grad_norm': 3.172382116317749, 'learning_rate': 8.379005690326445e-06, 'epoch': 1.23}
{'loss': 0.9882, 'grad_norm': 2.759021282196045, 'learning_rate': 8.341569332135371e-06, 'epoch': 1.25}
{'loss': 0.9941, 'grad_norm': 3.109468936920166, 'learning_rate': 8.304132973944295e-06, 'epoch': 1.26}
{'loss': 0.9886, 'grad_norm': 2.775512456893921, 'learning_rate': 8.26669661575322e-06, 'epoch': 1.28}
{'loss': 0.9903, 'grad_norm': 2.9725594520568848, 'learning_rate': 8.229260257562145e-06, 'epoch': 1.3}
{'loss': 0.9865, 'grad_norm': 3.4253392219543457, 'learning_rate': 8.19182389937107e-06, 'epoch': 1.31}
{'loss': 0.9751, 'grad_norm': 3.058499813079834, 'learning_rate': 8.154387541179994e-06, 'epoch': 1.33}
{'loss': 0.9743, 'grad_norm': 2.7487473487854004, 'learning_rate': 8.11695118298892e-06, 'epoch': 1.35}
{'eval_loss': 0.9982293248176575, 'eval_runtime': 185.5244, 'eval_samples_per_second': 53.901, 'eval_steps_per_second': 0.846, 'epoch': 1.35}
{'loss': 0.9799, 'grad_norm': 3.296046257019043, 'learning_rate': 8.079514824797844e-06, 'epoch': 1.36}
{'loss': 0.9767, 'grad_norm': 2.4221184253692627, 'learning_rate': 8.042078466606768e-06, 'epoch': 1.38}
{'loss': 0.9756, 'grad_norm': 2.86156964302063, 'learning_rate': 8.004642108415695e-06, 'epoch': 1.4}
{'loss': 0.9701, 'grad_norm': 2.6653635501861572, 'learning_rate': 7.96720575022462e-06, 'epoch': 1.42}
{'loss': 0.9754, 'grad_norm': 3.0187108516693115, 'learning_rate': 7.929769392033543e-06, 'epoch': 1.43}
{'loss': 0.9753, 'grad_norm': 2.521178722381592, 'learning_rate': 7.892333033842469e-06, 'epoch': 1.45}
{'loss': 0.9723, 'grad_norm': 3.237353801727295, 'learning_rate': 7.854896675651393e-06, 'epoch': 1.47}
{'loss': 0.9697, 'grad_norm': 2.680438756942749, 'learning_rate': 7.817460317460318e-06, 'epoch': 1.48}
{'loss': 0.9691, 'grad_norm': 3.0457537174224854, 'learning_rate': 7.780023959269242e-06, 'epoch': 1.5}
{'loss': 0.9649, 'grad_norm': 2.260953903198242, 'learning_rate': 7.742587601078168e-06, 'epoch': 1.52}
{'loss': 0.965, 'grad_norm': 2.363551378250122, 'learning_rate': 7.705151242887092e-06, 'epoch': 1.53}
{'loss': 0.9684, 'grad_norm': 2.3649871349334717, 'learning_rate': 7.667714884696018e-06, 'epoch': 1.55}
{'loss': 0.9669, 'grad_norm': 2.2331953048706055, 'learning_rate': 7.630278526504942e-06, 'epoch': 1.57}
{'loss': 0.9579, 'grad_norm': 2.4398488998413086, 'learning_rate': 7.592842168313867e-06, 'epoch': 1.58}
{'loss': 0.9593, 'grad_norm': 2.41436767578125, 'learning_rate': 7.555405810122792e-06, 'epoch': 1.6}
{'loss': 0.956, 'grad_norm': 2.568967580795288, 'learning_rate': 7.517969451931717e-06, 'epoch': 1.62}
{'loss': 0.956, 'grad_norm': 2.3293416500091553, 'learning_rate': 7.480533093740642e-06, 'epoch': 1.63}
{'loss': 0.9523, 'grad_norm': 2.0516016483306885, 'learning_rate': 7.4430967355495665e-06, 'epoch': 1.65}
{'loss': 0.9559, 'grad_norm': 2.89070987701416, 'learning_rate': 7.405660377358491e-06, 'epoch': 1.67}
{'loss': 0.9542, 'grad_norm': 2.9119246006011963, 'learning_rate': 7.368224019167416e-06, 'epoch': 1.68}
{'eval_loss': 0.968601644039154, 'eval_runtime': 185.9695, 'eval_samples_per_second': 53.772, 'eval_steps_per_second': 0.844, 'epoch': 1.68}
{'loss': 0.9539, 'grad_norm': 2.3695623874664307, 'learning_rate': 7.330787660976341e-06, 'epoch': 1.7}
{'loss': 0.9544, 'grad_norm': 2.7757413387298584, 'learning_rate': 7.293351302785266e-06, 'epoch': 1.72}
{'loss': 0.9537, 'grad_norm': 2.0786776542663574, 'learning_rate': 7.2559149445941905e-06, 'epoch': 1.74}
{'loss': 0.9482, 'grad_norm': 2.1129395961761475, 'learning_rate': 7.2184785864031145e-06, 'epoch': 1.75}
{'loss': 0.9537, 'grad_norm': 3.028501510620117, 'learning_rate': 7.181042228212041e-06, 'epoch': 1.77}
{'loss': 0.9508, 'grad_norm': 2.5789883136749268, 'learning_rate': 7.143605870020966e-06, 'epoch': 1.79}
{'loss': 0.9457, 'grad_norm': 2.295550584793091, 'learning_rate': 7.10616951182989e-06, 'epoch': 1.8}
{'loss': 0.9497, 'grad_norm': 1.8411766290664673, 'learning_rate': 7.0687331536388145e-06, 'epoch': 1.82}
{'loss': 0.948, 'grad_norm': 3.0015697479248047, 'learning_rate': 7.031296795447739e-06, 'epoch': 1.84}
{'loss': 0.9452, 'grad_norm': 2.4494314193725586, 'learning_rate': 6.993860437256664e-06, 'epoch': 1.85}
{'loss': 0.9441, 'grad_norm': 2.2997424602508545, 'learning_rate': 6.956424079065589e-06, 'epoch': 1.87}
{'loss': 0.9436, 'grad_norm': 2.2493653297424316, 'learning_rate': 6.918987720874514e-06, 'epoch': 1.89}
{'loss': 0.9437, 'grad_norm': 2.067580223083496, 'learning_rate': 6.8815513626834386e-06, 'epoch': 1.9}
{'loss': 0.9446, 'grad_norm': 2.279141902923584, 'learning_rate': 6.844115004492363e-06, 'epoch': 1.92}
{'loss': 0.9378, 'grad_norm': 2.0176503658294678, 'learning_rate': 6.806678646301288e-06, 'epoch': 1.94}
{'loss': 0.9445, 'grad_norm': 1.787514090538025, 'learning_rate': 6.769242288110213e-06, 'epoch': 1.95}
{'loss': 0.9403, 'grad_norm': 2.1923274993896484, 'learning_rate': 6.731805929919139e-06, 'epoch': 1.97}
{'loss': 0.9389, 'grad_norm': 2.5216240882873535, 'learning_rate': 6.6943695717280634e-06, 'epoch': 1.99}
{'loss': 0.9357, 'grad_norm': 1.8062264919281006, 'learning_rate': 6.656933213536988e-06, 'epoch': 2.0}
{'loss': 0.9216, 'grad_norm': 2.225322723388672, 'learning_rate': 6.619496855345913e-06, 'epoch': 2.02}
{'eval_loss': 0.9506262540817261, 'eval_runtime': 185.1752, 'eval_samples_per_second': 54.003, 'eval_steps_per_second': 0.848, 'epoch': 2.02}
{'loss': 0.9212, 'grad_norm': 1.770293116569519, 'learning_rate': 6.582060497154837e-06, 'epoch': 2.04}
{'loss': 0.9247, 'grad_norm': 1.8578999042510986, 'learning_rate': 6.544624138963762e-06, 'epoch': 2.06}
{'loss': 0.9216, 'grad_norm': 2.0663909912109375, 'learning_rate': 6.507187780772687e-06, 'epoch': 2.07}
{'loss': 0.9213, 'grad_norm': 1.905922770500183, 'learning_rate': 6.469751422581611e-06, 'epoch': 2.09}
{'loss': 0.9221, 'grad_norm': 1.8768377304077148, 'learning_rate': 6.432315064390536e-06, 'epoch': 2.11}
{'loss': 0.9198, 'grad_norm': 1.7672653198242188, 'learning_rate': 6.394878706199461e-06, 'epoch': 2.12}
{'loss': 0.9207, 'grad_norm': 2.1719400882720947, 'learning_rate': 6.357442348008386e-06, 'epoch': 2.14}
{'loss': 0.9223, 'grad_norm': 2.1967196464538574, 'learning_rate': 6.3200059898173115e-06, 'epoch': 2.16}
{'loss': 0.9187, 'grad_norm': 1.9893165826797485, 'learning_rate': 6.282569631626236e-06, 'epoch': 2.17}
{'loss': 0.9218, 'grad_norm': 1.7721937894821167, 'learning_rate': 6.245133273435161e-06, 'epoch': 2.19}
{'loss': 0.9194, 'grad_norm': 1.9004870653152466, 'learning_rate': 6.207696915244086e-06, 'epoch': 2.21}
{'loss': 0.9181, 'grad_norm': 1.671550989151001, 'learning_rate': 6.170260557053011e-06, 'epoch': 2.22}
{'loss': 0.919, 'grad_norm': 2.0082051753997803, 'learning_rate': 6.1328241988619355e-06, 'epoch': 2.24}
{'loss': 0.9188, 'grad_norm': 1.7042145729064941, 'learning_rate': 6.09538784067086e-06, 'epoch': 2.26}
{'loss': 0.917, 'grad_norm': 1.8694345951080322, 'learning_rate': 6.057951482479785e-06, 'epoch': 2.27}
{'loss': 0.9154, 'grad_norm': 1.8318318128585815, 'learning_rate': 6.020515124288709e-06, 'epoch': 2.29}
{'loss': 0.9186, 'grad_norm': 1.7127004861831665, 'learning_rate': 5.983078766097634e-06, 'epoch': 2.31}
{'loss': 0.9204, 'grad_norm': 1.8720226287841797, 'learning_rate': 5.945642407906559e-06, 'epoch': 2.32}
{'loss': 0.9182, 'grad_norm': 2.08514142036438, 'learning_rate': 5.9082060497154835e-06, 'epoch': 2.34}
{'loss': 0.9167, 'grad_norm': 1.9018237590789795, 'learning_rate': 5.870769691524409e-06, 'epoch': 2.36}
{'eval_loss': 0.9404377341270447, 'eval_runtime': 184.3599, 'eval_samples_per_second': 54.242, 'eval_steps_per_second': 0.852, 'epoch': 2.36}
{'loss': 0.9146, 'grad_norm': 1.7462258338928223, 'learning_rate': 5.833333333333334e-06, 'epoch': 2.38}
{'loss': 0.9194, 'grad_norm': 1.7853354215621948, 'learning_rate': 5.795896975142259e-06, 'epoch': 2.39}
{'loss': 0.9188, 'grad_norm': 2.0434694290161133, 'learning_rate': 5.7584606169511836e-06, 'epoch': 2.41}
{'loss': 0.9139, 'grad_norm': 1.541941523551941, 'learning_rate': 5.721024258760108e-06, 'epoch': 2.43}
{'loss': 0.9195, 'grad_norm': 1.6699239015579224, 'learning_rate': 5.683587900569033e-06, 'epoch': 2.44}
{'loss': 0.9141, 'grad_norm': 1.6724357604980469, 'learning_rate': 5.646151542377958e-06, 'epoch': 2.46}
{'loss': 0.9139, 'grad_norm': 1.536242961883545, 'learning_rate': 5.608715184186883e-06, 'epoch': 2.48}
{'loss': 0.9121, 'grad_norm': 1.5605733394622803, 'learning_rate': 5.571278825995808e-06, 'epoch': 2.49}
{'loss': 0.9155, 'grad_norm': 1.8133786916732788, 'learning_rate': 5.533842467804732e-06, 'epoch': 2.51}
{'loss': 0.9123, 'grad_norm': 1.6812946796417236, 'learning_rate': 5.496406109613656e-06, 'epoch': 2.53}
{'loss': 0.9154, 'grad_norm': 1.324622392654419, 'learning_rate': 5.458969751422583e-06, 'epoch': 2.54}
{'loss': 0.9141, 'grad_norm': 1.619882345199585, 'learning_rate': 5.421533393231508e-06, 'epoch': 2.56}
{'loss': 0.9153, 'grad_norm': 1.6744009256362915, 'learning_rate': 5.384097035040432e-06, 'epoch': 2.58}
{'loss': 0.9111, 'grad_norm': 1.506860613822937, 'learning_rate': 5.346660676849356e-06, 'epoch': 2.59}
{'loss': 0.9132, 'grad_norm': 1.5385479927062988, 'learning_rate': 5.309224318658281e-06, 'epoch': 2.61}
{'loss': 0.9128, 'grad_norm': 1.9707627296447754, 'learning_rate': 5.271787960467206e-06, 'epoch': 2.63}
{'loss': 0.9149, 'grad_norm': 1.4150334596633911, 'learning_rate': 5.234351602276131e-06, 'epoch': 2.64}
{'loss': 0.9121, 'grad_norm': 1.629360318183899, 'learning_rate': 5.196915244085056e-06, 'epoch': 2.66}
{'loss': 0.9107, 'grad_norm': 1.6054513454437256, 'learning_rate': 5.1594788858939804e-06, 'epoch': 2.68}
{'loss': 0.9106, 'grad_norm': 1.240429162979126, 'learning_rate': 5.122042527702905e-06, 'epoch': 2.7}
{'eval_loss': 0.9317799806594849, 'eval_runtime': 175.9661, 'eval_samples_per_second': 56.829, 'eval_steps_per_second': 0.892, 'epoch': 2.7}
{'loss': 0.9108, 'grad_norm': 1.4317172765731812, 'learning_rate': 5.08460616951183e-06, 'epoch': 2.71}
{'loss': 0.912, 'grad_norm': 1.5118372440338135, 'learning_rate': 5.047169811320756e-06, 'epoch': 2.73}
{'loss': 0.9063, 'grad_norm': 1.3612955808639526, 'learning_rate': 5.0097334531296805e-06, 'epoch': 2.75}
{'loss': 0.9083, 'grad_norm': 1.4235693216323853, 'learning_rate': 4.9722970949386045e-06, 'epoch': 2.76}
{'loss': 0.9082, 'grad_norm': 1.4688869714736938, 'learning_rate': 4.93486073674753e-06, 'epoch': 2.78}
{'loss': 0.9103, 'grad_norm': 1.6479781866073608, 'learning_rate': 4.897424378556455e-06, 'epoch': 2.8}
{'loss': 0.9063, 'grad_norm': 1.5083117485046387, 'learning_rate': 4.859988020365379e-06, 'epoch': 2.81}
{'loss': 0.9105, 'grad_norm': 1.6136168241500854, 'learning_rate': 4.822551662174304e-06, 'epoch': 2.83}
{'loss': 0.9094, 'grad_norm': 1.5181362628936768, 'learning_rate': 4.7851153039832285e-06, 'epoch': 2.85}
{'loss': 0.9091, 'grad_norm': 1.3786848783493042, 'learning_rate': 4.747678945792153e-06, 'epoch': 2.86}
{'loss': 0.9082, 'grad_norm': 1.5871566534042358, 'learning_rate': 4.710242587601079e-06, 'epoch': 2.88}
{'loss': 0.9064, 'grad_norm': 1.4558303356170654, 'learning_rate': 4.672806229410004e-06, 'epoch': 2.9}
{'loss': 0.9085, 'grad_norm': 1.1593401432037354, 'learning_rate': 4.6353698712189286e-06, 'epoch': 2.91}
{'loss': 0.9081, 'grad_norm': 1.1847045421600342, 'learning_rate': 4.597933513027853e-06, 'epoch': 2.93}
{'loss': 0.9079, 'grad_norm': 1.3311830759048462, 'learning_rate': 4.560497154836777e-06, 'epoch': 2.95}
{'loss': 0.9063, 'grad_norm': 2.099203586578369, 'learning_rate': 4.523060796645702e-06, 'epoch': 2.96}
{'loss': 0.9058, 'grad_norm': 1.3609787225723267, 'learning_rate': 4.485624438454628e-06, 'epoch': 2.98}
{'loss': 0.9053, 'grad_norm': 1.8574013710021973, 'learning_rate': 4.448188080263553e-06, 'epoch': 3.0}
{'loss': 0.8967, 'grad_norm': 1.3965362310409546, 'learning_rate': 4.410751722072477e-06, 'epoch': 3.02}
{'loss': 0.897, 'grad_norm': 1.3584798574447632, 'learning_rate': 4.373315363881402e-06, 'epoch': 3.03}
{'eval_loss': 0.9262301921844482, 'eval_runtime': 176.6553, 'eval_samples_per_second': 56.607, 'eval_steps_per_second': 0.889, 'epoch': 3.03}
{'loss': 0.8959, 'grad_norm': 1.2785314321517944, 'learning_rate': 4.335879005690327e-06, 'epoch': 3.05}
{'loss': 0.896, 'grad_norm': 1.1592707633972168, 'learning_rate': 4.298442647499252e-06, 'epoch': 3.07}
{'loss': 0.897, 'grad_norm': 1.5303773880004883, 'learning_rate': 4.261006289308177e-06, 'epoch': 3.08}
{'loss': 0.8964, 'grad_norm': 1.3681528568267822, 'learning_rate': 4.2235699311171014e-06, 'epoch': 3.1}
{'loss': 0.8946, 'grad_norm': 0.8100528717041016, 'learning_rate': 4.186133572926026e-06, 'epoch': 3.12}
{'loss': 0.8954, 'grad_norm': 1.046511173248291, 'learning_rate': 4.148697214734951e-06, 'epoch': 3.13}
{'loss': 0.8957, 'grad_norm': 1.325129508972168, 'learning_rate': 4.111260856543876e-06, 'epoch': 3.15}
{'loss': 0.8957, 'grad_norm': 1.233781337738037, 'learning_rate': 4.073824498352801e-06, 'epoch': 3.17}
{'loss': 0.8956, 'grad_norm': 1.1335160732269287, 'learning_rate': 4.0363881401617255e-06, 'epoch': 3.18}
{'loss': 0.8957, 'grad_norm': 1.4107532501220703, 'learning_rate': 3.99895178197065e-06, 'epoch': 3.2}
{'loss': 0.8962, 'grad_norm': 1.420893669128418, 'learning_rate': 3.961515423779575e-06, 'epoch': 3.22}
{'loss': 0.8946, 'grad_norm': 1.1522362232208252, 'learning_rate': 3.9240790655885e-06, 'epoch': 3.23}
{'loss': 0.8953, 'grad_norm': 1.3575130701065063, 'learning_rate': 3.886642707397425e-06, 'epoch': 3.25}
{'loss': 0.896, 'grad_norm': 1.2799293994903564, 'learning_rate': 3.8492063492063495e-06, 'epoch': 3.27}
{'loss': 0.8935, 'grad_norm': 1.2776434421539307, 'learning_rate': 3.8117699910152743e-06, 'epoch': 3.29}
{'loss': 0.8926, 'grad_norm': 1.1695204973220825, 'learning_rate': 3.774333632824199e-06, 'epoch': 3.3}
{'loss': 0.8942, 'grad_norm': 1.8023604154586792, 'learning_rate': 3.736897274633124e-06, 'epoch': 3.32}
{'loss': 0.8948, 'grad_norm': 1.1401244401931763, 'learning_rate': 3.6994609164420487e-06, 'epoch': 3.34}
{'loss': 0.8951, 'grad_norm': 0.9871705770492554, 'learning_rate': 3.662024558250974e-06, 'epoch': 3.35}
{'loss': 0.8943, 'grad_norm': 1.6214840412139893, 'learning_rate': 3.6245882000598987e-06, 'epoch': 3.37}
{'eval_loss': 0.9210482835769653, 'eval_runtime': 173.7521, 'eval_samples_per_second': 57.553, 'eval_steps_per_second': 0.904, 'epoch': 3.37}
{'loss': 0.8944, 'grad_norm': 1.1995747089385986, 'learning_rate': 3.5871518418688235e-06, 'epoch': 3.39}
{'loss': 0.8919, 'grad_norm': 1.1713744401931763, 'learning_rate': 3.549715483677748e-06, 'epoch': 3.4}
{'loss': 0.8927, 'grad_norm': 0.769260048866272, 'learning_rate': 3.5122791254866727e-06, 'epoch': 3.42}
{'loss': 0.8929, 'grad_norm': 1.0522936582565308, 'learning_rate': 3.4748427672955975e-06, 'epoch': 3.44}
{'loss': 0.8939, 'grad_norm': 1.0662568807601929, 'learning_rate': 3.4374064091045228e-06, 'epoch': 3.45}
{'loss': 0.8936, 'grad_norm': 1.1652339696884155, 'learning_rate': 3.3999700509134476e-06, 'epoch': 3.47}
{'loss': 0.8919, 'grad_norm': 1.3109698295593262, 'learning_rate': 3.3625336927223724e-06, 'epoch': 3.49}
{'loss': 0.8917, 'grad_norm': 1.0119926929473877, 'learning_rate': 3.325097334531297e-06, 'epoch': 3.5}
{'loss': 0.8924, 'grad_norm': 1.0015864372253418, 'learning_rate': 3.2876609763402216e-06, 'epoch': 3.52}
{'loss': 0.8902, 'grad_norm': 1.0155137777328491, 'learning_rate': 3.2502246181491464e-06, 'epoch': 3.54}
{'loss': 0.8945, 'grad_norm': 0.9289907813072205, 'learning_rate': 3.2127882599580716e-06, 'epoch': 3.55}
{'loss': 0.8911, 'grad_norm': 1.3473438024520874, 'learning_rate': 3.1753519017669964e-06, 'epoch': 3.57}
{'loss': 0.8927, 'grad_norm': 1.2974601984024048, 'learning_rate': 3.1379155435759212e-06, 'epoch': 3.59}
{'loss': 0.8919, 'grad_norm': 0.7495006918907166, 'learning_rate': 3.100479185384846e-06, 'epoch': 3.61}
{'loss': 0.8921, 'grad_norm': 0.8633655309677124, 'learning_rate': 3.063042827193771e-06, 'epoch': 3.62}
{'loss': 0.893, 'grad_norm': 1.255744218826294, 'learning_rate': 3.025606469002695e-06, 'epoch': 3.64}
{'loss': 0.8907, 'grad_norm': 0.9510488510131836, 'learning_rate': 2.988170110811621e-06, 'epoch': 3.66}
{'loss': 0.8906, 'grad_norm': 0.9992819428443909, 'learning_rate': 2.9507337526205452e-06, 'epoch': 3.67}
{'loss': 0.8908, 'grad_norm': 1.1475728750228882, 'learning_rate': 2.91329739442947e-06, 'epoch': 3.69}
{'loss': 0.8893, 'grad_norm': 0.9765082001686096, 'learning_rate': 2.875861036238395e-06, 'epoch': 3.71}
{'eval_loss': 0.9171139597892761, 'eval_runtime': 173.3586, 'eval_samples_per_second': 57.684, 'eval_steps_per_second': 0.906, 'epoch': 3.71}
{'loss': 0.8897, 'grad_norm': 1.0198628902435303, 'learning_rate': 2.8384246780473197e-06, 'epoch': 3.72}
{'loss': 0.891, 'grad_norm': 0.9789551496505737, 'learning_rate': 2.800988319856245e-06, 'epoch': 3.74}
{'loss': 0.8906, 'grad_norm': 1.2772375345230103, 'learning_rate': 2.7635519616651697e-06, 'epoch': 3.76}
{'loss': 0.8896, 'grad_norm': 0.8041599988937378, 'learning_rate': 2.7261156034740945e-06, 'epoch': 3.77}
{'loss': 0.8892, 'grad_norm': 1.1759151220321655, 'learning_rate': 2.688679245283019e-06, 'epoch': 3.79}
{'loss': 0.8896, 'grad_norm': 0.8636193871498108, 'learning_rate': 2.6512428870919437e-06, 'epoch': 3.81}
{'loss': 0.8912, 'grad_norm': 1.0439530611038208, 'learning_rate': 2.6138065289008685e-06, 'epoch': 3.82}
{'loss': 0.891, 'grad_norm': 0.9883546233177185, 'learning_rate': 2.5763701707097937e-06, 'epoch': 3.84}
{'loss': 0.8895, 'grad_norm': 0.9972785115242004, 'learning_rate': 2.5389338125187185e-06, 'epoch': 3.86}
{'loss': 0.8894, 'grad_norm': 0.9896754622459412, 'learning_rate': 2.5014974543276433e-06, 'epoch': 3.87}
{'loss': 0.8872, 'grad_norm': 0.7018574476242065, 'learning_rate': 2.464061096136568e-06, 'epoch': 3.89}
{'loss': 0.8908, 'grad_norm': 0.9972265958786011, 'learning_rate': 2.426624737945493e-06, 'epoch': 3.91}
{'loss': 0.8881, 'grad_norm': 0.8484935164451599, 'learning_rate': 2.3891883797544177e-06, 'epoch': 3.93}
{'loss': 0.8883, 'grad_norm': 1.2606288194656372, 'learning_rate': 2.3517520215633426e-06, 'epoch': 3.94}
{'loss': 0.8889, 'grad_norm': 0.9781013131141663, 'learning_rate': 2.3143156633722674e-06, 'epoch': 3.96}
{'loss': 0.891, 'grad_norm': 0.726010799407959, 'learning_rate': 2.276879305181192e-06, 'epoch': 3.98}
{'loss': 0.889, 'grad_norm': 1.0505636930465698, 'learning_rate': 2.239442946990117e-06, 'epoch': 3.99}
{'loss': 0.8867, 'grad_norm': 0.858825147151947, 'learning_rate': 2.2020065887990418e-06, 'epoch': 4.01}
{'loss': 0.8811, 'grad_norm': 0.7274307608604431, 'learning_rate': 2.1645702306079666e-06, 'epoch': 4.03}
{'loss': 0.8816, 'grad_norm': 0.7780688405036926, 'learning_rate': 2.1271338724168914e-06, 'epoch': 4.04}
{'eval_loss': 0.9144274592399597, 'eval_runtime': 174.4596, 'eval_samples_per_second': 57.32, 'eval_steps_per_second': 0.9, 'epoch': 4.04}
{'loss': 0.8808, 'grad_norm': 0.9795958995819092, 'learning_rate': 2.089697514225816e-06, 'epoch': 4.06}
{'loss': 0.8821, 'grad_norm': 1.0119067430496216, 'learning_rate': 2.052261156034741e-06, 'epoch': 4.08}
{'loss': 0.8812, 'grad_norm': 0.8838882446289062, 'learning_rate': 2.014824797843666e-06, 'epoch': 4.09}
{'loss': 0.8793, 'grad_norm': 2.39704966545105, 'learning_rate': 1.9773884396525906e-06, 'epoch': 4.11}
{'loss': 0.881, 'grad_norm': 1.1133852005004883, 'learning_rate': 1.9399520814615154e-06, 'epoch': 4.13}
{'loss': 0.8803, 'grad_norm': 0.8877719044685364, 'learning_rate': 1.9025157232704406e-06, 'epoch': 4.14}
{'loss': 0.8823, 'grad_norm': 0.9075058698654175, 'learning_rate': 1.8650793650793652e-06, 'epoch': 4.16}
{'loss': 0.8801, 'grad_norm': 0.9550015330314636, 'learning_rate': 1.82764300688829e-06, 'epoch': 4.18}
{'loss': 0.8822, 'grad_norm': 0.8999351859092712, 'learning_rate': 1.790206648697215e-06, 'epoch': 4.19}
{'loss': 0.8816, 'grad_norm': 1.1859521865844727, 'learning_rate': 1.7527702905061397e-06, 'epoch': 4.21}
{'loss': 0.8806, 'grad_norm': 1.019366979598999, 'learning_rate': 1.7153339323150645e-06, 'epoch': 4.23}
{'loss': 0.8823, 'grad_norm': 1.04983651638031, 'learning_rate': 1.6778975741239895e-06, 'epoch': 4.25}
{'loss': 0.8805, 'grad_norm': 1.060006856918335, 'learning_rate': 1.6404612159329143e-06, 'epoch': 4.26}
{'loss': 0.8806, 'grad_norm': 1.0279498100280762, 'learning_rate': 1.6030248577418389e-06, 'epoch': 4.28}
{'loss': 0.8805, 'grad_norm': 0.8781208395957947, 'learning_rate': 1.5655884995507639e-06, 'epoch': 4.3}
{'loss': 0.881, 'grad_norm': 0.7265690565109253, 'learning_rate': 1.5281521413596887e-06, 'epoch': 4.31}
{'loss': 0.8807, 'grad_norm': 0.9014593362808228, 'learning_rate': 1.4907157831686133e-06, 'epoch': 4.33}
{'loss': 0.8805, 'grad_norm': 0.9795975685119629, 'learning_rate': 1.4532794249775383e-06, 'epoch': 4.35}
{'loss': 0.8789, 'grad_norm': 0.8940288424491882, 'learning_rate': 1.4158430667864631e-06, 'epoch': 4.36}
{'loss': 0.8803, 'grad_norm': 0.9475533962249756, 'learning_rate': 1.378406708595388e-06, 'epoch': 4.38}
{'eval_loss': 0.9120455980300903, 'eval_runtime': 173.8372, 'eval_samples_per_second': 57.525, 'eval_steps_per_second': 0.903, 'epoch': 4.38}
{'loss': 0.8816, 'grad_norm': 1.2060487270355225, 'learning_rate': 1.340970350404313e-06, 'epoch': 4.4}
{'loss': 0.8788, 'grad_norm': 0.9045171141624451, 'learning_rate': 1.3035339922132375e-06, 'epoch': 4.41}
{'loss': 0.8802, 'grad_norm': 0.7777021527290344, 'learning_rate': 1.2660976340221623e-06, 'epoch': 4.43}
{'loss': 0.8801, 'grad_norm': 0.7898547053337097, 'learning_rate': 1.2286612758310874e-06, 'epoch': 4.45}
{'loss': 0.879, 'grad_norm': 0.8159648776054382, 'learning_rate': 1.191224917640012e-06, 'epoch': 4.46}
{'loss': 0.8792, 'grad_norm': 0.7850489616394043, 'learning_rate': 1.153788559448937e-06, 'epoch': 4.48}
{'loss': 0.8799, 'grad_norm': 1.001803994178772, 'learning_rate': 1.1163522012578618e-06, 'epoch': 4.5}
{'loss': 0.878, 'grad_norm': 0.8213627934455872, 'learning_rate': 1.0789158430667866e-06, 'epoch': 4.51}
{'loss': 0.8793, 'grad_norm': 0.7480529546737671, 'learning_rate': 1.0414794848757114e-06, 'epoch': 4.53}
{'loss': 0.8799, 'grad_norm': 0.9619048237800598, 'learning_rate': 1.0040431266846362e-06, 'epoch': 4.55}
{'loss': 0.8804, 'grad_norm': 0.8296887874603271, 'learning_rate': 9.66606768493561e-07, 'epoch': 4.57}
{'loss': 0.8786, 'grad_norm': 1.055829644203186, 'learning_rate': 9.291704103024858e-07, 'epoch': 4.58}
{'loss': 0.8795, 'grad_norm': 0.7763046622276306, 'learning_rate': 8.917340521114107e-07, 'epoch': 4.6}
{'loss': 0.8788, 'grad_norm': 0.8022878766059875, 'learning_rate': 8.542976939203355e-07, 'epoch': 4.62}
{'loss': 0.8794, 'grad_norm': 0.8619744777679443, 'learning_rate': 8.168613357292603e-07, 'epoch': 4.63}
{'loss': 0.8796, 'grad_norm': 0.6904770731925964, 'learning_rate': 7.794249775381851e-07, 'epoch': 4.65}
{'loss': 0.8783, 'grad_norm': 0.9032456874847412, 'learning_rate': 7.4198861934711e-07, 'epoch': 4.67}
{'loss': 0.8785, 'grad_norm': 1.1301318407058716, 'learning_rate': 7.045522611560348e-07, 'epoch': 4.68}
{'loss': 0.8768, 'grad_norm': 0.9860104322433472, 'learning_rate': 6.671159029649596e-07, 'epoch': 4.7}
{'loss': 0.878, 'grad_norm': 0.9878997802734375, 'learning_rate': 6.296795447738845e-07, 'epoch': 4.72}
{'eval_loss': 0.9100372195243835, 'eval_runtime': 170.5677, 'eval_samples_per_second': 58.628, 'eval_steps_per_second': 0.92, 'epoch': 4.72}
{'loss': 0.8784, 'grad_norm': 0.9252628684043884, 'learning_rate': 5.922431865828093e-07, 'epoch': 4.73}
{'loss': 0.8769, 'grad_norm': 1.1024415493011475, 'learning_rate': 5.548068283917342e-07, 'epoch': 4.75}
{'loss': 0.8777, 'grad_norm': 0.7802477478981018, 'learning_rate': 5.173704702006589e-07, 'epoch': 4.77}
{'loss': 0.8764, 'grad_norm': 0.9537503123283386, 'learning_rate': 4.799341120095837e-07, 'epoch': 4.78}
{'loss': 0.8793, 'grad_norm': 0.9035133719444275, 'learning_rate': 4.424977538185086e-07, 'epoch': 4.8}
{'loss': 0.8809, 'grad_norm': 0.9949420094490051, 'learning_rate': 4.050613956274334e-07, 'epoch': 4.82}
{'loss': 0.8785, 'grad_norm': 0.9288338422775269, 'learning_rate': 3.6762503743635825e-07, 'epoch': 4.83}
{'loss': 0.8782, 'grad_norm': 0.8269996047019958, 'learning_rate': 3.3018867924528305e-07, 'epoch': 4.85}
{'loss': 0.8797, 'grad_norm': 0.7920528054237366, 'learning_rate': 2.9275232105420786e-07, 'epoch': 4.87}
{'loss': 0.8778, 'grad_norm': 0.7805834412574768, 'learning_rate': 2.553159628631327e-07, 'epoch': 4.89}
{'loss': 0.8779, 'grad_norm': 0.973613977432251, 'learning_rate': 2.1787960467205755e-07, 'epoch': 4.9}
{'loss': 0.8778, 'grad_norm': 0.806638240814209, 'learning_rate': 1.8044324648098233e-07, 'epoch': 4.92}
{'loss': 0.8767, 'grad_norm': 0.9608660340309143, 'learning_rate': 1.4300688828990716e-07, 'epoch': 4.94}
{'loss': 0.878, 'grad_norm': 0.7596495151519775, 'learning_rate': 1.0557053009883199e-07, 'epoch': 4.95}
{'loss': 0.8771, 'grad_norm': 1.1000720262527466, 'learning_rate': 6.813417190775682e-08, 'epoch': 4.97}
{'loss': 0.879, 'grad_norm': 0.8734977841377258, 'learning_rate': 3.069781371668165e-08, 'epoch': 4.99}
{'train_runtime': 138374.7376, 'train_samples_per_second': 6.865, 'train_steps_per_second': 0.107, 'train_loss': 1.0683724169461233, 'epoch': 5.0}

 If there's a warning about missing keys above, please disregard :)

 If there's a warning about missing keys above, please disregard :)

 If there's a warning about missing keys above, please disregard :)
 If there's a warning about missing keys above, please disregard :)

[1;34mwandb[0m: 🚀 View run [33m/data/user_data/jingyuah/LLARA/checkpoints/pixel_200k_user_token[0m at: [34mhttps://wandb.ai/jingyuanhe1222/LLARA_pretrain/runs/cu7cooy0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241110_222833-cu7cooy0/logs[0m
