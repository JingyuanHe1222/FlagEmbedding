{
  "os":  "Linux-5.14.0-427.40.1.el9_4.x86_64-x86_64-with-glibc2.34",
  "python":  "3.10.15",
  "startedAt":  "2024-11-29T00:36:30.272345Z",
  "args":  [
    "--output_dir",
    "/data/user_data/jingyuah/LLARA/checkpoints/amazon_books_item_EBAE_5e-6",
    "--model_name_or_path",
    "/data/user_data/jingyuah/HLLM_weights/checkpoints/TinyLlama-1.1B-intermediate-step-1431k-3T",
    "--train_data",
    "/data/user_data/jingyuah/LLARA/data/pretrain/amazon_books_item.jsonl",
    "--learning_rate",
    "5e-6",
    "--num_train_epochs",
    "10",
    "--per_device_train_batch_size",
    "128",
    "--per_device_eval_batch_size",
    "128",
    "--evaluation_strategy",
    "steps",
    "--metric_for_best_model",
    "eval_loss",
    "--load_best_model_at_end",
    "True",
    "--gradient_accumulation_steps",
    "1",
    "--dataloader_drop_last",
    "True",
    "--cutoff_len",
    "256",
    "--logging_steps",
    "5",
    "--save_steps",
    "100",
    "--eval_steps",
    "100",
    "--save_total_limit",
    "2",
    "--gradient_checkpointing",
    "--ddp_find_unused_parameters",
    "False",
    "--use_flash_attn",
    "False",
    "--deepspeed",
    "../stage1.json",
    "--warmup_ratio",
    "0.1",
    "--remove_stop_words",
    "True",
    "--use_lora",
    "False",
    "--bf16",
    "--cache_dir",
    "/data/user_data/jingyuah/LLARA/checkpoints/amazon_books_item_EBAE_5e-6",
    "--token",
    "True",
    "--report_to",
    "wandb"
  ],
  "program":  "/home/jingyuah/FlagEmbedding/research/LLARA/pretrain/run.py",
  "codePath":  "research/LLARA/pretrain/run.py",
  "git":  {
    "remote":  "https://github.com/JingyuanHe1222/FlagEmbedding.git",
    "commit":  "d76e51ca59c85e81d722245ad0605e8f14f5a167"
  },
  "email":  "jingyuanhe1222@gmail.com",
  "root":  "/home/jingyuah/FlagEmbedding/research/LLARA/pretrain",
  "host":  "babel-0-19",
  "username":  "jingyuah",
  "executable":  "/home/jingyuah/miniconda3/envs/llara/bin/python",
  "codePathLocal":  "run.py",
  "cpu_count":  128,
  "cpu_count_logical":  256,
  "gpu":  "NVIDIA RTX A6000",
  "gpu_count":  2,
  "disk":  {
    "/":  {
      "total":  "236663791616",
      "used":  "94514700288"
    }
  },
  "memory":  {
    "total":  "1081442340864"
  },
  "cpu":  {
    "count":  128,
    "countLogical":  256
  },
  "gpu_nvidia":  [
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    }
  ],
  "slurm":  {
    "cluster_name":  "babel",
    "conf":  "/var/spool/slurmd/conf-cache/slurm.conf",
    "cpu_bind":  "quiet,mask_cpu:0x00000000000000010000000000000001",
    "cpu_bind_list":  "0x00000000000000010000000000000001",
    "cpu_bind_type":  "mask_cpu:",
    "cpu_bind_verbose":  "quiet",
    "cpus_on_node":  "2",
    "cpus_per_task":  "1",
    "distribution":  "cyclic",
    "gpus_on_node":  "2",
    "gtids":  "0",
    "job_account":  "cx",
    "job_cpus_per_node":  "2",
    "job_end_time":  "1733013367",
    "job_gid":  "2701553",
    "job_gpus":  "2,4",
    "job_group":  "jingyuah",
    "job_id":  "3468833",
    "job_name":  "amzn_item_tinyllama_5e-6",
    "job_nodelist":  "babel-0-19",
    "job_num_nodes":  "1",
    "job_partition":  "general",
    "job_qos":  "normal",
    "job_start_time":  "1732840567",
    "job_uid":  "2701553",
    "job_user":  "jingyuah",
    "jobid":  "3468833",
    "launch_node_ipaddr":  "172.16.1.4",
    "localid":  "0",
    "mem_per_node":  "65536",
    "nnodes":  "1",
    "nodeid":  "0",
    "nodelist":  "babel-0-19",
    "nprocs":  "1",
    "ntasks":  "1",
    "ntasks_per_node":  "1",
    "prio_process":  "0",
    "procid":  "0",
    "pty_port":  "43845",
    "pty_win_col":  "150",
    "pty_win_row":  "17",
    "script_context":  "prolog_task",
    "srun_comm_host":  "172.16.1.4",
    "srun_comm_port":  "39105",
    "step_id":  "0",
    "step_launcher_port":  "39105",
    "step_nodelist":  "babel-12-17",
    "step_num_nodes":  "1",
    "step_num_tasks":  "1",
    "step_tasks_per_node":  "1",
    "stepid":  "0",
    "submit_dir":  "/home/jingyuah/FlagEmbedding/research/LLARA/scripts",
    "submit_host":  "babel-12-17",
    "task_pid":  "3165656",
    "tasks_per_node":  "1",
    "topology_addr":  "babel-0-19",
    "topology_addr_pattern":  "node",
    "tres_per_task":  "cpu=1"
  },
  "cudaVersion":  "12.6"
}