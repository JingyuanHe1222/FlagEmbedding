{
  "os":  "Linux-5.14.0-427.40.1.el9_4.x86_64-x86_64-with-glibc2.34",
  "python":  "3.10.15",
  "startedAt":  "2024-11-29T19:53:32.010809Z",
  "args":  [
    "--output_dir",
    "/data/user_data/jingyuah/LLARA/checkpoints/amazon_books_pythia_1b_lr_5e-6",
    "--model_name_or_path",
    "EleutherAI/pythia-1b",
    "--train_data",
    "/data/user_data/jingyuah/LLARA/data/pretrain/amazon_books_item_96k.jsonl",
    "--learning_rate",
    "5e-6",
    "--num_train_epochs",
    "10",
    "--per_device_train_batch_size",
    "128",
    "--per_device_eval_batch_size",
    "128",
    "--evaluation_strategy",
    "steps",
    "--metric_for_best_model",
    "eval_loss",
    "--load_best_model_at_end=True",
    "--gradient_accumulation_steps",
    "1",
    "--dataloader_drop_last",
    "True",
    "--cutoff_len",
    "128",
    "--logging_steps",
    "5",
    "--save_steps",
    "100",
    "--eval_steps",
    "100",
    "--save_total_limit",
    "2",
    "--gradient_checkpointing",
    "--ddp_find_unused_parameters",
    "False",
    "--use_flash_attn",
    "False",
    "--deepspeed",
    "../stage1.json",
    "--warmup_ratio",
    "0.1",
    "--remove_stop_words",
    "True",
    "--use_lora",
    "False",
    "--bf16",
    "--cache_dir",
    "/data/user_data/jingyuah/LLARA/checkpoints/amazon_books_pythia_1b_lr_5e-6",
    "--token",
    "True",
    "--report_to",
    "wandb"
  ],
  "program":  "/home/jingyuah/FlagEmbedding/research/LLARA/pretrain/ntp_run.py",
  "codePath":  "research/LLARA/pretrain/ntp_run.py",
  "git":  {
    "remote":  "https://github.com/JingyuanHe1222/FlagEmbedding.git",
    "commit":  "d76e51ca59c85e81d722245ad0605e8f14f5a167"
  },
  "email":  "jingyuanhe1222@gmail.com",
  "root":  "/home/jingyuah/FlagEmbedding/research/LLARA/pretrain",
  "host":  "babel-0-31",
  "username":  "jingyuah",
  "executable":  "/home/jingyuah/miniconda3/envs/llara/bin/python",
  "codePathLocal":  "ntp_run.py",
  "cpu_count":  128,
  "cpu_count_logical":  256,
  "gpu":  "NVIDIA RTX A6000",
  "gpu_count":  2,
  "disk":  {
    "/":  {
      "total":  "236663791616",
      "used":  "78931968000"
    }
  },
  "memory":  {
    "total":  "1081442328576"
  },
  "cpu":  {
    "count":  128,
    "countLogical":  256
  },
  "gpu_nvidia":  [
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    },
    {
      "name":  "NVIDIA RTX A6000",
      "memoryTotal":  "51527024640",
      "cudaCores":  10752,
      "architecture":  "Ampere"
    }
  ],
  "slurm":  {
    "cluster_name":  "babel",
    "conf":  "/var/spool/slurmd/conf-cache/slurm.conf",
    "cpus_on_node":  "2",
    "cpus_per_task":  "1",
    "gpus_on_node":  "2",
    "gtids":  "0",
    "job_account":  "cx",
    "job_cpus_per_node":  "2",
    "job_end_time":  "1733082716",
    "job_gid":  "2701553",
    "job_gpus":  "2,7",
    "job_id":  "3505293",
    "job_name":  "amzn_item_128_pythia1b_5e-6",
    "job_nodelist":  "babel-0-31",
    "job_num_nodes":  "1",
    "job_partition":  "general",
    "job_qos":  "normal",
    "job_start_time":  "1732909916",
    "job_uid":  "2701553",
    "job_user":  "jingyuah",
    "jobid":  "3505293",
    "localid":  "0",
    "mem_per_node":  "65536",
    "nnodes":  "1",
    "nodeid":  "0",
    "nodelist":  "babel-0-31",
    "nprocs":  "1",
    "ntasks":  "1",
    "ntasks_per_node":  "1",
    "prio_process":  "0",
    "procid":  "0",
    "script_context":  "prolog_task",
    "submit_dir":  "/home/jingyuah/FlagEmbedding/research/LLARA/scripts",
    "submit_host":  "babel-login-2",
    "task_pid":  "338862",
    "tasks_per_node":  "1",
    "topology_addr":  "babel-0-31",
    "topology_addr_pattern":  "node",
    "tres_per_task":  "cpu=1"
  },
  "cudaVersion":  "12.6"
}